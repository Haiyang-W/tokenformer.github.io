<!doctype html>
<html lang="en">
    <head>
        <title>TokenFormer: A Fully Attention-based Neural Network for Foundation Models</title>
        <link rel="icon" type="image/x-icon" href="/static/img/icons/jellyfish.ico">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://cambrian-mllm.github.io/" />
        <meta property="og:image" content="https://cambrian-mllm.github.io/static/img/preview.png" />
        <meta property="og:title" content="Cambrian-1: A Fully Open Vision-Centric Exploration of MLLMs" />
        <meta property="og:description" content="Cambrian-1 is a family of multimodal LLMs with a vision-centric design. We also release CV-Bench, a new vision-centric benchmark, and Cambrian-10M, a multimodal instruction-tuning dataset." />
        
        <!-- Twitter -->
        <meta name="twitter:url" content="https://cambrian-mllm.github.io/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="https://cambrian-mllm.github.io/static/img/preview.png" />
        <meta name="twitter:title" content="Cambrian-1: A Fully Open Vision-Centric Exploration of MLLMs" />
        <meta name="twitter:description" content="Cambrian-1 is a family of multimodal LLMs with a vision-centric design. We also release CV-Bench, a new vision-centric benchmark, and Cambrian-10M, a multimodal instruction-tuning dataset." />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>TokenFormer</i></h1>
                    <h2>Rethinking Transformer Scaling with Tokenized Model Parameters<br></h2>
                        <p>
                            <em><strong>TokenFormer is a fully attention-based neural network that unifies the computations of token-token and token-parameter
                            interactions by entirely employing the attention mechanism, 
                            maximizes the flexibility of neural network</strong></em>.&emsp;
                            <br>
                            <br>
                            It not only tokenizes data but also model parameters, 
                            replacing the model concept with interaction flows between data and parameter tokens.
                            This makes it can handle a variable number of parameters, offers greater flexibility than traditional Transformers,
                            will further contribute to the development of FMs in the future as follows:
                        </p>
                    
                        <div class="icon-container">
                            <div class="icon-item">
                                <img src="./static/img/icons/visual.svg" alt="Visual Representation Icon">
                                <div><strong>Incremental Model Scaling</strong>: Our model allows for progressive scaling without retraining from scratch by adding new key-value parameter pairs, greatly reduces training costs. <em><strong>This advantage is the primary focus in this paper.</strong></em></div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/connector.svg" alt="Connector Design Icon">
                                <div><strong>Device-Cloud Collaboration</strong>: It can serve as the cloud knowledge base in Device-Cloud Collaboration, 
                                    with each pair of key-value parameter tokens representing a learnable pattern. </div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/data.svg" alt="Instruction Tuning Data Icon">
                                <div><strong>Sparse Inference (MoE)</strong>: We interpret Tokenformer as an extreme Mixture of Experts (MoE), with each 
                                    key-value pair acting as an expert, greatly reduces inference costs. </div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/recipe.svg" alt="Instruction Tuning Recipes Icon">
                                <div><strong>Parameter Efficient Tuning</strong>: When confronted with new tasks or datasets, 
                                    the model can augment its pre-trained parameters by
                                    incorporating these new parameter tokens, 
                                    thereby adapting to specific task requirements quickly. </div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/connector.svg" alt="Benchmarking Icon">
                                <div><strong>Integrating Vision and Language Models.</strong>: Enabling seamless 
                                    integration of visual-language by merging key-value parameter tokens 
                                    from pre-trained visual and language models. </div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/eval.svg" alt="Benchmarking Icon">
                                <div><strong>Model Interpretability</strong>: Itâ€™s attention-based design naturally enhances model interpretability.</div>
                            </div>
                        </div>
                        <div class="button-container">
                            <!-- replace arxiv -->
                            <a href="https://github.com/Haiyang-W/TokenFormer" class="button paper-link" target="_blank">
                                <span class="icon is-small">
                                    <i class="ai ai-arxiv"></i>
                                </span>
                                arXiv
                            </a>
                            <!-- replace pdf -->
                            <a href="https://github.com/Haiyang-W/TokenFormer" class="button paper-link" target="_blank">
                                <span class="icon is-small">
                                    <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>pdf</span>
                            </a>
                            <!-- replace image -->
                            <a href="https://github.com/Haiyang-W/TokenFormer" class="button" target="_blank">
                                <span class="icon is-small">
                                    <i class="fab fa-github"></i>
                                </span>
                                <span>Code</span>
                            </a>
                            <!-- <br> -->
                            <a href="https://github.com/Haiyang-W/TokenFormer" class="button" target="_blank">
                                <span class="icon is-small">
                                    <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1em;">
                                </span>
                                <span>Checkpoints</span>
                            </a>
                        </div>
                </div>
                <div class="header-image">
                    <img draggable="false" src="static/img/comparision.png" alt="Teaser Image" class="teaser-image">
                    <img draggable="false" src="static/img/incremental_scaling.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://haiyang-w.github.io/" class="author-link" target="_blank">Haiyang Wang<sup>1,3</sup></a> &emsp;
                    <a href="https://yue-fan.github.io/" class="author-link" target="_blank">Yue Fan<sup>1<sup></a> &emsp;
                    <a href="https://ferjad.github.io/" class="author-link" target="_blank">Muhammad Ferjad Naeem<sup>2</sup></a> &emsp;
                    <a href="https://xianyongqin.github.io/" class="author-link" target="_blank">Yongqin Xian<sup>2</sup></a> &emsp;
                    <!-- <br> -->
                    <a href="https://janericlenssen.github.io/" class="author-link" target="_blank">Jan Eric Lenssen<sup>1</sup></a> &emsp;
                    <a href="http://www.liweiwang-pku.com/" class="author-link" target="_blank">Liwei Wang<sup>3</sup></a> &emsp;
                    <a href="https://federicotombari.github.io/" class="author-link" target="_blank">Federico Tombari<sup>2</sup></a> &emsp;
                    <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/people/bernt-schiele" class="author-link" target="_blank">Bernt Schiele<sup>1</sup></a> &emsp;
                    <p></p>
                    <sup>1</sup><a href="https://www.mpi-inf.mpg.de/home" class="affiliation-link" target="_blank">Max Planck Institute for Informatics</a> &emsp;
                    <sup>2</sup><a href="https://about.google/" class="affiliation-link" target="_blank">Google</a> &emsp;
                    <sup>3</sup><a href="https://english.pku.edu.cn/" class="affiliation-link" target="_blank">Peking University</a> &emsp;
                </p>
            </div>
        </div>
        

        
        <p class="text abstract">
            We introduce Tokenformer, a <strong><i>fully attention-based architecture</i></strong> that unifies the computations 
            of token-token and token-parameter interactions by entirely employing the attention mechanism, 
            <strong>maximizes the flexibility of neural network</strong>. The advantage makes it can handle a variable number of parameters, 
            inherently enhances the model's scalability, facilitating progressively efficient scaling.
        </p>

        <p class="text abstract">
            Hope that this architecture can offer greater flexibility than traditional Transformers,
             will further contribute to the development of foundation models, sparse inference (MoE), 
             parameter efficient tuning, device-cloud collaboration, vision-language, model interpretability, and so on.
        </p>

        

        <hr>

        <div id='method' class="method-block">

            <h1 class="text">Token-Parameter Attention</h1>
            <p class="text">
                Although transformers excel across various domains, their scalability is limited by high computational overheads resulting from prescribed token-parameter 
                interactions (i.e., linear projections). As a result, scaling strategies that adjust architectural components(e.g., channel dimensions) typically require 
                retraining the entire model from the beginning, leading to inefficient use of computational resources.  
                <br>
                <br>
                To overcome this challenge, we propose TokenFormer, an architecture entirely based on attention mechanisms. The central innovation of TokenFormer is token-<strong>P</strong>arameter <strong>attention</strong> (Pattention) layer, which incorporates a set of trainable tokens functioning as model parameters and then employs cross-attention to manage interactions between input tokens and these parameter tokens.
                <!-- <ol class="text">
                    <li>We encode spatial inductive bias by explicitly localizing the aggregation space for each token in the query.</li>
                    <li>We perform vision feature aggregation multiple times across the LLM layers, allowing the model to repeatedly reference necessary visual information.</li>
                </ol> -->
            </p>
            <d-figure id="fig-pattention">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/token-parameter_attention.png" alt="Token-Parameter Attention">
                    <figcaption style="text-align: left;">
                        <strong>Figure 1:</strong> TokenFormer is a fully attention-driven architecture featuring a new token-parameter attention (Pattention) layer. 
                        The Pattention uses a set of learnable tokens to represent model parameters and lets the input tokens attend to them.
                    </figcaption>
                </figure>
            </d-figure>
            <p class="text">
                <strong>Pattention.</strong> To implement our Pattention mechanism, we use inputs as query and introduce two sets of
                n learnable parameter tokens to represent the keys and values. 
                The output from the scaled dot-product Pattention layer is computed as:
            </p>
            <d-figure id="eq1">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/pattention.png" alt="pattention function">
                </figure>
            </d-figure>
            <p class="text">
                where &#920; is a modified softmax operation for
                stable optimization of Pattention layer. The output Pattention scores are formulated as,
            </p>
            <d-figure id="eq1">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/softmax.png" alt="modified softmax function">
                </figure>
            </d-figure>
            <p class="text">
                where A is the attention score, &#964; is the scale factor, and f is a non-linearity function, which in our formulation is set to the GeLU function. 
                This design improves gradient stability in our architecture and results in better performance compared to the standard softmax operation.
                <br>
                <br>
                <strong>Overall Architecture. </strong> The above figure illustrates the architecture of Tokenformer. 
                We follow the standard transformer and replace all the linear projections with our Pattention layers. 
                By designing the architecture in this manner, we represent all fundamental components-including both
                input data and model parametersâ€”as tokens within the computational framework, thereby establishing a
                fully attention-based neural network characterized by exceptional flexibility.
            </p>

        </div>

        <div id='method' class="method-block">

            <h1 class="text">Progressive model scaling</h1>
            <p class="text">
                Consider an existing Tokenformer model equipped with a set of pre-trained key-value parameter tokens,
                we augment this set by appending new key-value parameter tokens.
            </p>
            <d-figure id="fig-pattention">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/model_scaling.png" alt="Progressive model scaling">
                    <figcaption style="text-align: left;">
                        <strong>Figure 2:</strong> As the model scales, TokenFormer adds new learnable tokens to expand the 
                        existing key-value parameter sets, while keeping the feature dimension constant and leaving the 
                        rest of the computation unaffected.
                    </figcaption>
                </figure>
            </d-figure>
            <p class="text">
                This scaling scheme permits the integration of an arbitrary number of parameters without altering the
                input or output dimension.
            </p>
        </div>

        <hr>

        <div id='method' class="method-block">

            <h1 class="text">Experiments</h1>
            <p class="text">
                <strong>Progressive Model Scaling.</strong> Our progressive scaling methodology employing
                Tokenformer achieves performance comparable to that of a Transformer model trained from scratch,
                while substantially reducing the training budget.
            </p>
            <d-figure id="fig-pattention">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/progressive_model_scaling.png" alt="Progressive model scaling">
                    <figcaption style="text-align: left;">
                        <strong>Figure 3:</strong> <strong>Left:</strong> Evaluating model scaling costs through
                        cumulative computational budgets. The Transformer baseline incurs expenses 
                        for each individual scaling step performed independently from
                        scratch, whereas Tokenformer aggregates costs
                        across all scaling stages, including training a
                        124M model initially, progressively scaling to
                        354M, 757M, and 1.4B parameters. 
                        <strong>Right:</strong> Evaluating model scaling costs by measuring the budget required at each scaling stage.
                        The Transformer baselines used are consistent
                        with those depicted in Figure 3, trained with 30B
                        and 300B tokens. Similarly, for Tokenformer, the
                        cost is the budget required for each incremental
                        scaling step from a smaller one. All the experiments were conducted on TPU v4 hardware.
                    </figcaption>
                </figure>
            </d-figure>
            <p class="text">
                <strong>Language Modeling.</strong> The below table presents the performance of 
                Tokenformer across various widely-recognized zero-shot downstream tasks. 
                Comparisons are drawn against leading open-source transformer models of equivalent
                scale. As shown in this table, our model achieves competitive
                performance compared to the standard Transformer, demonstrating the potential of our architecture
                in terms of expressive power as a foundation model.
            </p>
            <d-figure id="fig-pattention">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/language_model.png" alt="Progressive model scaling">
                    <figcaption style="text-align: left;">
                        <strong>Table 1:</strong> <strong>Zero-shot Evaluation:</strong> The best performance for each model size is highlighted in bold.
                        Our comparisons are made with publicly available transformer-based LMs with various tokenizers.
                        Following Pythia, our model is trained for up to 300B tokens on pile dataset.
                    </figcaption>
                </figure>
            </d-figure>
            <p class="text">
                <strong>Vision Modeling.</strong> The below table validates the expressiveness of our model in visual tasks. 
                We compare our approach against the standard Vision Transformer (ViT) trained with
                supervised learning on the ImageNet-1K dataset. As shown in the table, our model achieves the same
                performance as ViT in visual modeling, confirming its expressiveness in visual tasks.
            </p>
            <d-figure id="fig-pattention">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/vision_model.png" alt="Progressive model scaling">
                    <figcaption style="text-align: left;">
                        <strong>Table 2:</strong> <strong>ImageNet-1K Classification:</strong> Comparison of standard vision transformer on ImageNet-1K. The
                        training hyperparameters are completely consistent (batch size, learning rate, etc.) with MAE. â€  denotes models where the parameter size has been matched to that of the standard ViT.
                    </figcaption>
                </figure>
            </d-figure>
        </div>

        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                This paper introduces Tokenformer, a naturally scalable architecture that leverages the attention
                mechanism to facilitate not only inter-token computations but also interactions between tokens and
                model parameters, thereby enhancing architectural flexibility. By representing model parameters as
                tokens, we replace all linear projection layers in the Transformer with our Pattention layers, allowing
                for seamless and efficient incremental scaling without the need for retraining from scratch. We believe
                that this architecture, offering greater flexibility than traditional Transformers, will further contribute
                to the development of foundation models.
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{haiyang2024tokenformer,<br>
                &nbsp;&nbsp;title={{TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters}},<br>
                &nbsp;&nbsp;author={Haiyang Wang, Yue Fan, Muhammad Ferjad Naeem, Liwei Wang, Yongqin Xian, Jan Eric Lenssen, Federico Tombari, Bernt Schiele},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:2406.16860},<br>
                &nbsp;&nbsp;year={2024}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>   
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
    </body>
</html>
